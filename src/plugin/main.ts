
import type {AugmentedEnum, AugmentedMessage} from './env';

import type {Dict} from '@blake.regalia/belt';


import {entries, flatten_entries, transform_object, values} from '@blake.regalia/belt';

import {NeutrinoImpl} from './impl-neutrino';
import {plugin} from './plugin';
import {arrayLit, arrayType, castAs, declareAlias, declareConst, funcType, ident, importModule, keyword, objectLit, param, print, tuple, typeRef} from './ts-factory';

// inject into preamble of any file, allow linter to delete unused imports
const A_GLOBAL_PREAMBLE = [
	`/*
	* ${'='.repeat(32)}
	*     GENERATED FILE WARNING
	* Do not edit this file manually.
	* ${'='.repeat(32)}
	*/`.replace(/\n\t+/g, '\n'),
	...[
		importModule('@blake.regalia/belt', [
			'F_IDENTITY',
			'__UNDEFINED',
			'assign',
			'base64_to_bytes',
			'text_to_bytes',
		]),
		importModule('@blake.regalia/belt', [
			'NaiveHexLower',
		], true),
		importModule('@solar-republic/types', [
			'WeakInt64Str',
			'WeakUint64Str',
			'WeakInt128Str',
			'WeakUint128Str',
			'WeakAccountAddr',
			'WeakValidatorAddr',
			'SlimCoin',
			'CwInt64',
			'CwUint64',
			'CwHexLower',
			'CwBase64',
			'CwAccountAddr',
			'CwValidatorAddr',
		], true),
		importModule('@solar-republic/crypto', [
			'bech32_encode',
			'bech32_decode',
		]),
		importModule('#/api/util', [
			'safe_bytes_to_base64',
			'safe_bytes_to_text',
			'safe_base64_to_bytes',
			'safe_base64_to_text',
			'addr_bytes_to_bech32',
		]),
		importModule('#/api/json', [
			'parse_duration',
			'parse_timestamp',
			'duration_to_json',
			'timestamp_to_json',
			'expand_timestamp',
			'expand_duration',
			'expand_coin',
		]),
		importModule('#/api/protobuf-writer', [
			'Protobuf',
			'map',
			'apply_opt',
			'temporal',
			'any',
			'coin',
			'coins',
			'slimify_coin',
		]),
		importModule('#/api/protobuf-reader', [
			'decode_protobuf',
			'decode_protobuf_r0',
			'decode_protobuf_r0_0',
			'decode_coin',
			'reduce_temporal',
			'decode_temporal',
		]),
		importModule('#/api/transport', [
			'F_RPC_REQ_NO_ARGS',
			'restful_grpc',
			'restruct_coin',
			'restruct_temporal',
		]),
		importModule('#/api/types', [
			'Encoded',
			'JsonAny',
			'Opt',
			'WeakTimestampStr',
			'WeakDurationStr',
		], true),
	].map(yn => print(yn)),
];


export const main = () => {
	void plugin((a_protos, h_inputs, h_types, h_interfaces, h_params) => {
		// new impl
		const k_impl = new NeutrinoImpl(h_types, h_interfaces, h_params);

		const h_drafts: Dict<{
			enums: Set<string>;
		}> = {};

		const h_outputs: Dict<string> = {};

		type SerializedMessages = Dict<{
			message: AugmentedMessage;
			contents: string[];
		}>;

		// [typePath: string]: serialized message
		const h_encoders: SerializedMessages = {};
		const h_decoders: SerializedMessages = {};
		const h_destructors: SerializedMessages = {};
		const h_accessors: SerializedMessages = {};
		const h_condensers: SerializedMessages = {};
		const h_expanders: SerializedMessages = {};

		// 
		type ClosureStruct = {
			messages: Dict<AugmentedMessage>;
			enums: Dict<AugmentedEnum>;
		};

		const init_closure = (): ClosureStruct => ({
			messages: {},
			enums: {},
		});

		const g_encoders = init_closure();
		const g_decoders = init_closure();
		const g_destructors = init_closure();
		const g_accessible = init_closure();

		// const h_closure: Dict<AugmentedMessage> = {};
		// const h_enums: Dict<AugmentedEnum> = {};

		/**
		 * add closures from given message type
		 */
		function mark_fields(g_msg: AugmentedMessage, g_coders: ClosureStruct) {
			// ensure message is covered
			g_coders.messages[g_msg.path] = g_msg;

			// ensure every field's type has an encoder
			for(const g_field of g_msg.fieldList) {
				// ref type path
				const si_type = g_field.typeName;

				// no typename; skip
				if(!si_type) continue;

				// resolve type
				const g_resolved = h_types[si_type];

				// type exists, is not encoded, and is not yet defined in closure/enum
				if(!g_coders.messages[si_type] && !g_coders.enums[si_type]) {
					// message; recurse
					if('message' === g_resolved.form) {
						mark_fields(g_resolved, g_coders);
					}
					// enum
					else {
						// save to enums struct
						g_coders.enums[si_type] = g_resolved;

						// TODO: what should happen here between encoders and decoders?

						// if('ProposalStatus' === g_msg.source.name) debugger;

						// add to draft
						(h_drafts[g_resolved.source.name!] ||= {
							enums: new Set(),
						}).enums.add(si_type);
					}
				}
			}
		}


		const h_tmps: Dict<{
			a_gateways: string[];
			a_anys: string[];
			a_decoders: string[];
		}> = {};

		// each proto file
		for(const g_proto of a_protos) {
			// open file for path
			k_impl.open(g_proto);

			// new string lists
			const a_gateways: string[] = [];
			const a_anys: string[] = [];
			const a_decoders: string[] = [];

			h_tmps[k_impl.path] = {
				a_gateways,
				a_anys,
				a_decoders,
			};

			// each top-level messages
			for(const g_msg of g_proto.messageTypeList) {
				// ref message options
				const g_opts = g_msg.options;

				// implements interface
				if(g_opts?.implementsInterfaceList) {
					// produce 'any' encoder
					a_anys.push(k_impl.anyEncoder(g_msg));

					// ensure its fields can be encoded
					mark_fields(g_msg, g_encoders);

					// ensure its fields can be described
					mark_fields(g_msg, g_accessible);

					// ensure response values can be destructured
					mark_fields(g_msg, g_destructors);
				}

				// each encoder pattern
				for(const r_match of h_params.encoders || []) {
					// match found
					if(r_match.test(g_msg.path.slice(1))) {
						// verbose
						console.warn(`INFO: Matched to encoders pattern: "${g_msg.path.slice(1)}"`);

						// add encoder
						h_encoders[g_msg.path] = {
							message: g_msg,
							contents: k_impl.msgEncoder(g_msg),
						};

						// ensure its fields can be encoded
						mark_fields(g_msg, g_encoders);

						// ensure its fields can be accessed for condenser
						mark_fields(g_msg, g_accessible);
					}
				}

				// each decoder pattern
				for(const r_match of h_params.decoders || []) {
					// match found
					if(r_match.test(g_msg.path.slice(1))) {
						// verbose
						console.warn(`INFO: Matched to decoders pattern: "${g_msg.path.slice(1)}"`);

						// add decoder
						h_decoders[g_msg.path] = {
							message: g_msg,
							contents: k_impl.msgDecoder(g_msg),
						};

						// ensure its fields can be decoded
						mark_fields(g_msg, g_decoders);

						// ensure its fields can be accessed for expander
						mark_fields(g_msg, g_accessible);
					}
				}

				// each destructor pattern
				for(const r_match of h_params.destructors || []) {
					// match found
					if(r_match.test(g_msg.path.slice(1))) {
						// verbose
						console.warn(`INFO: Matched to destructors pattern: "${g_msg.path.slice(1)}"`);

						// add encoder
						h_destructors[g_msg.path] = {
							message: g_msg,
							contents: k_impl.msgDestructor(g_msg),
						};

						// ensure its fields can be destructed
						mark_fields(g_msg, g_destructors);
					}
				}

				// each accessor pattern
				for(const r_match of h_params.accessors || []) {
					// match found
					if(r_match.test(g_msg.path.slice(1))) {
						// verbose
						console.warn(`INFO: Matched to accessors pattern: "${g_msg.path.slice(1)}"`);

						// add accessor
						h_accessors[g_msg.path] = {
							message: g_msg,
							contents: k_impl.msgAccessor(g_msg),
						};

						// ensure its fields can be accessed
						mark_fields(g_msg, g_accessible);
					}
				}
			}

			// each service
			for(const g_service of g_proto.serviceList) {
				// find longest path prefix
				let s_path_prefix = '';
				{
					// each method (pre-gen)
					for(const g_method of g_service.methodList) {
						const g_opts = g_method.options;
						const g_http = g_opts?.http;

						// only interested in GET or POST
						const sr_path = g_http?.get || g_http?.post;

						// no path; skip
						if(!sr_path) continue;

						// initialize
						if(!s_path_prefix) {
							s_path_prefix = sr_path;
							const i_param = sr_path.indexOf('{');
							if(i_param >= 0) s_path_prefix = sr_path.slice(0, sr_path.indexOf('{'));
						}
						// find common substring
						else {
							// each character
							for(let i_char=0; i_char<Math.min(s_path_prefix.length, sr_path.length); i_char++) {
								// end of common substring
								if(sr_path[i_char] !== s_path_prefix[i_char]) {
									s_path_prefix = sr_path.slice(0, i_char);
									break;
								}
							}
						}
					}
				}

				// each method
				for(const g_method of g_service.methodList) {
					// skip functions missing input or output type
					if(!g_method.inputType || !g_method.outputType) continue;
					const {
						inputType: sr_input,
						outputType: sr_output,
					} = g_method;

					// locate rpc input
					const g_input = h_types[sr_input];
					if('message' !== g_input?.form) throw new Error(`Failed to find rpc input ${sr_input} in ${g_proto.name!}`);

					// locate rpc output
					const g_output = h_types[sr_output];
					if('message' !== g_output?.form) throw new Error(`Failed to find rpc output ${sr_output}`);

					// gRPC-gateway method
					const g_http = g_method.options?.http;
					if(g_http?.get || g_http?.post) {
						const sx_gateway = k_impl.gateway(
							s_path_prefix,
							g_method,
							g_input,
							g_output
						);

						// add gateway method
						a_gateways.push(sx_gateway);

						// ensure its output can be destructured
						mark_fields(g_output, g_destructors);

						// ensure its output can be described
						mark_fields(g_output, g_accessible);

						// ensure its inputs are typed and enums accessible
						mark_fields(g_input, g_accessible);
					}
					// otherwise, ensure user can encode method inputs and decode method outputs
					else {
						// add input encoder
						h_encoders[g_input.path] = {
							message: g_input,
							contents: k_impl.msgEncoder(g_input),
						};

						// ensure its fields can be encoded
						mark_fields(g_input, g_encoders);

						// ensure its inputs are typed and enums accessible
						mark_fields(g_input, g_accessible);

						// method output has content
						if(g_output.fieldList.length) {
							// add decoder
							a_decoders.push(...k_impl.msgDecoder(g_output));

							// ensure its fields can be decoded
							mark_fields(g_output, g_decoders);
						}
					}
				}
			}
		}


		// each message in need of an encoder
		for(const [, g_msg] of entries(g_encoders.messages)) {
			// add encoder
			h_encoders[g_msg.path] = {
				message: g_msg,
				contents: k_impl.msgEncoder(g_msg),
			};

			// add condenser
			h_condensers[g_msg.path] = {
				message: g_msg,
				contents: k_impl.msgCondenser(g_msg),
			};
		}

		debugger;
		// each message in need of a decoder
		for(const [, g_msg] of entries(g_decoders.messages)) {
			// add decoder
			h_decoders[g_msg.path] = {
				message: g_msg,
				contents: k_impl.msgDecoder(g_msg),
			};

			// add expander
			h_expanders[g_msg.path] = {
				message: g_msg,
				contents: k_impl.msgExpander(g_msg),
			};

			const a_destructor = k_impl.msgDestructor(g_msg);
			if(a_destructor) {
				h_destructors[g_msg.path] = {
					message: g_msg,
					contents: a_destructor,
				};
			}
		}

		// each message in need of a destructor
		for(const [, g_msg] of entries(g_destructors.messages)) {
			const a_destructor = k_impl.msgDestructor(g_msg);
			if(a_destructor) {
				h_destructors[g_msg.path] = {
					message: g_msg,
					contents: a_destructor,
				};
			}
		}

		// each message in need of an accessor
		for(const [, g_msg] of entries(g_accessible.messages)) {
			const a_accessors = k_impl.msgAccessor(g_msg);
			if(a_accessors) {
				h_accessors[g_msg.path] = {
					message: g_msg,
					contents: a_accessors,
				};
			}
		}

		// condensers
		{
			// open file
			k_impl.open(NeutrinoImpl.condenserFile(), true);
			const p_output = k_impl.path;

			for(const g_encoder of values(h_encoders)) {
				k_impl.importConstant(g_encoder.message, 'decode');
			}

			// prep file parts
			const g_parts = {
				head: [
					...A_GLOBAL_PREAMBLE,
					`import type {JsonObject} from '@blake.regalia/belt';`,
					...k_impl.imports(),
				],
				body: flatten_entries(h_condensers, (sr, g_condenser) => g_condenser.contents),
			};

			// save output
			h_outputs[p_output] = [
				[...new Set(g_parts.head)].join('\n'),
				...g_parts.body,

				...[
					declareAlias('Expander', funcType(
						[param('a_decoded', arrayType(keyword('any')))],
						typeRef('JsonObject')
					)),

					declareAlias('Decoder', funcType(
						[param('atu8_payload', typeRef('Uint8Array'))],
						arrayType(keyword('any'))
					)),

					declareConst(`H_REGISTRY_ANY`, objectLit(transform_object(h_encoders, (si, g_encoder) => {
						const g_msg = g_encoder.message;

						return {
							[g_msg.path.replace(/^\./, '/')]: arrayLit([
								ident(`condense${k_impl.clashFreeTypeId(g_msg)}`),
								castAs(ident(`expand${k_impl.clashFreeTypeId(g_msg)}`), typeRef('Expander')),
								ident(`decode${k_impl.clashFreeTypeId(g_msg)}`),
							]),
						};
					})), false, typeRef('Record', [
						keyword('string'),
						tuple([
							funcType([param('g_msg', typeRef('JsonObject'))], typeRef('Uint8Array')),
							typeRef('Expander'),
							typeRef('Decoder'),
						]),
					])),
				].map(yn => print(yn)),

				`
					export const condenseJsonAny = (g_any: JsonAny | undefined, p_type: string|undefined=g_any?.['@type']): Uint8Array | undefined => g_any? encodeGoogleProtobufAny(p_type!, H_REGISTRY_ANY[p_type!][0](g_any)): __UNDEFINED;

					export const expandJsonAny = <
						p_type extends string=string,
						g_msg extends JsonObject=JsonObject,
					>([s_type_url, atu8_value]: [
						s_type_url?: p_type,
						atu8_value?: Uint8Array,
					]): JsonAny<p_type, g_msg> => ({
						'@type': s_type_url!,
						...H_REGISTRY_ANY[s_type_url!][2](atu8_value!) as unknown as g_msg,
					});

					//export const expandProtobufAny = (atu8_any: DecodedGoogleProtobufAny | undefined): JsonAny | undefined => g_any? expandGoogleProtobufAny(p_type!, H_REGISTRY_ANY[p_type!][1](g_any)): __UNDEFINED;
				`,

			].join('\n\n');
		}

		debugger;
		// every file
		for(const [, g_proto] of entries(h_inputs)) {
			// open
			k_impl.open(g_proto);
			const p_output = k_impl.path;

			const {
				a_gateways,
				a_anys,
			} = h_tmps[k_impl.path] || {};

			// body
			const a_body: string[] = [];

			// prep file parts
			const g_parts = {
				head: [
					...A_GLOBAL_PREAMBLE,
					...k_impl.imports(),
				],
				body: a_body,
			};

			// lcd methods
			if(a_gateways?.length) {
				g_parts.head.push(...k_impl.head('lcd'));

				a_body.push(
					...k_impl.body('lcd'),
					...a_gateways
				);
			}

			// anys
			if(a_anys?.length) {
				g_parts.head.push(...k_impl.head('any'));

				a_body.push(
					...k_impl.body('any'),
					...a_anys
				);
			}

			// encoders
			const a_encoders: string[] = [];
			for(const g_encoder of values(h_encoders)) {
				if(g_proto === g_encoder.message.source) {
					a_encoders.push(...g_encoder.contents);
				}
			}

			if(a_encoders.length) {
				g_parts.head.push(...k_impl.head('encoder'));

				a_body.push(
					...k_impl.body('encoder'),
					...a_encoders
				);
			}

			// decoders
			const a_decoders: string[] = [];
			for(const g_decoder of values(h_decoders)) {
				if(g_proto === g_decoder.message.source) {
					a_decoders.push(...g_decoder.contents);
				}
			}

			if(a_decoders.length) {
				g_parts.head.push(...k_impl.head('decoder'));

				a_body.push(
					...k_impl.body('decoder'),
					...a_decoders
				);
			}

			// destructors
			const a_destructors: string[] = [];
			for(const g_destructor of values(h_destructors)) {
				if(g_proto === g_destructor.message.source) {
					a_destructors.push(...g_destructor.contents);
				}
			}

			if(a_destructors.length) {
				a_body.push(
					...a_destructors
				);
			}


			// accessors
			const a_accessors: string[] = [];
			for(const [p_accessor, g_accessor] of entries(h_accessors)) {
				// skip redundant
				if(h_destructors[p_accessor]) continue;

				if(g_proto === g_accessor.message.source) {
					a_accessors.push(...g_accessor.contents);
				}
			}

			if(a_accessors.length) {
				a_body.push(
					...a_accessors
				);
			}

			// // decoders
			// if(a_decoders?.length) {
			// 	g_parts.head.push(...k_impl.head('decoder'));

			// 	a_body.push(
			// 		...k_impl.body('decoder'),
			// 		...a_decoders
			// 	);
			// }


			const g_draft = h_drafts[g_proto.name!] || {};

			// enums
			const as_enums = g_draft.enums;
			if(as_enums) {
				for(const si_enum of as_enums) {
					const g_enum = h_types[si_enum] as AugmentedEnum;

					a_body.push(...k_impl.enums(g_enum));
				}
			}

			// there are contents to write to the file
			if(a_body.length) {
				h_outputs[p_output] = [[...new Set(g_parts.head)].join('\n'), ...g_parts.body].join('\n\n');
			}
		}

		// // global registries
		// {
		// 	function create_any_registry() {
		// 		k_impl.open({
		// 			name: '._any_condense',
		// 		});

		// 		const s_prefix = 'condense';
		// 		const s_dep = 'encode';

		// 		const h_imports: Record<string, ImportSpecifier[]> = {};
		// 		const h_import_types: Record<string, ImportSpecifier[]> = {};
		// 		const a_condensers: string[] = [];

		// 		for(const [, g_encoder] of ode(h_encoders)) {
		// 			const g_msg = g_encoder.message;
		// 			const sr_import = `#/proto/${map_proto_path(g_msg.source)}`;

		// 			(h_imports[sr_import] ??= []).push(...[
		// 				'encode',
		// 			].map(s_pre => y_factory.createImportSpecifier(
		// 				false,
		// 				ident(`${s_pre}${k_impl.exportedId(g_msg)}`),
		// 				ident(`${s_pre}${k_impl.clashFreeTypeId(g_msg)}`)
		// 			)));

		// 			(h_import_types[sr_import] ??= []).push(...[
		// 				'',
		// 				'Encoded',
		// 			].map(s_pre => y_factory.createImportSpecifier(
		// 				false,
		// 				ident(`${s_pre}${k_impl.exportedId(g_msg)}`),
		// 				ident(`${s_pre}${k_impl.clashFreeTypeId(g_msg)}`)
		// 			)));

		// 			a_condensers.push(k_impl.msgCondenser(g_msg));
		// 		}

		// 		h_outputs[`_any_${s_prefix}.ts`] = [
		// 			...A_GLOBAL_PREAMBLE,

		// 			...[
		// 				...ode(h_import_types).sort(([sr_a], [sr_b]) => sr_a === sr_b? 0: sr_a < sr_b? -1: 1)
		// 					.map(([sr_import, a_imports]) => importModule(sr_import, a_imports, true)),

		// 				...ode(h_imports).sort(([sr_a], [sr_b]) => sr_a === sr_b? 0: sr_a < sr_b? -1: 1)
		// 					.map(([sr_import, a_imports]) => importModule(sr_import, a_imports)),
		// 			].map(yn => print(yn)),

		// 			...a_condensers,

		// 			...[
		// 				declareConst(`H_REGISTRY_ANY_${s_prefix.toUpperCase()}`, objectLit(oderom(h_encoders, (si, g_encoder) => {
		// 					const g_msg = g_encoder.message;

		// 					return {
		// 						[g_msg.path.replace(/^\./, '/')]: ident(`${s_prefix}${k_impl.clashFreeTypeId(g_msg)}`),
		// 					};
		// 				})), false, typeRef('Record', [
		// 					// typeRef('MessageType'),
		// 					keyword('string'),
		// 					funcType([param('g_msg', typeRef('JsonObject'))], typeRef('Uint8Array')),
		// 				])),
		// 			].map(yn => print(yn)),

		// 			`
		// 				export const ${s_prefix}JsonAny = (g_any: JsonAny | undefined, p_type: string|undefined=g_any?.['@type']): Uint8Array | undefined => g_any? encodeGoogleProtobufAny(p_type!, H_REGISTRY_ANY_${s_prefix.toUpperCase()}[p_type!](g_any)): __UNDEFINED;
		// 			`,
		// 		].join('\n\n').replace(/\n+\nimport/g, '\nimport');
		// 	}

		// 	// create_any_registry('encode');
		// 	create_any_registry();
		// }

		return h_outputs;
	});
};
